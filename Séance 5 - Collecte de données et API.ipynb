{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Séance 5 - Collecte de données et API\n",
    "\n",
    "L'objectif de la séance est de voir un peu de la collecte de données sur internet, et pour cela de commencer à se familiariser avec la notion d'API.\n",
    "\n",
    "L'idée étant que beaucoup de données existantes doivent être acquises en s'interfaçant avec différents systèmes : serveurs, bases de données, etc. Tout cela conduit à devoir maîtriser différentes formes d'API\n",
    "\n",
    "Quatre moments :\n",
    "- Ce qu'est une API\n",
    "- Une API avec clé d'accès : le cas de Twitter\n",
    "- Traiter des données du web sans API\n",
    "- Quelques notions plus avancées sur les API\n",
    "\n",
    "https://medium.com/@perrysetgo/what-exactly-is-an-api-69f36968a41f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moment 1 - Ce qu'est une API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est quoi une API web, les données sur internet, etc.\n",
    "\n",
    "Une API (interface de programmation d’application) est un ensemble de conventions explicite ou implicite permettant a un ou des ordinateurs/programmes de communiquer. \n",
    "\n",
    "Ces conventions sont importantes afin de developper en isolation relative, et/ou de réutilser/changer des bibliothèques. Ce qui fait (ou ne fait pas partie) d'une API peut parfois sembler subjectif.\n",
    "\n",
    "Example \"d'API\" dans la vie de tous les jours.\n",
    " - Boulons/Écroux : Le filetage est standard\n",
    " - Container\n",
    " - Code Barre.\n",
    " - 220V \n",
    " - On conduit a droite (du moins en europe)\n",
    " - Hocher la tête pour dire \"oui\"\n",
    " - Vert genéralement Positif, Rouge généralement Négatif\n",
    " - Les pièces de monaies.\n",
    " - ...\n",
    " \n",
    " \n",
    "Bien qu'aucune des conventions si-dessus soit nécéssaire pour une société bien huilée, il est généralement préférable de suivre ces conventions. Quelles conventions sont suivies peut parvois dépendre du context, et des personnes avec qui on interagis.\n",
    "\n",
    "Une API en programation est similaire dans le sens ou :\n",
    "  - La définition exacte de API va dépendre du context ou l'on se place avec qui où quoi on communique\n",
    "      - Aux US la notion de se \"faire la bise\" est une notion boolean\n",
    "      - En Europe, ça fait être combien, par quel coté en commence.\n",
    "  - Tant que possible change peu\n",
    "  \n",
    "Cependant un API en programation was souvent avoir beaucoup plus de details dans ses valeurs, et va être plus stricte; étant prévu majoritairement pour communication machine <-> machine, (avec parfois le programmeur), la verbosité n'est pas un problème.\n",
    "\n",
    "Une API va souvent aussi contenir les donnés brutes, par example \"2020-05-09T13:49:54+00:00\" au lieu de \"9 May 2020\".\n",
    "\n",
    "Les documentations D'API sont pratiquement toujours très abstraites, et la documentation va être assez générique.\n",
    "\n",
    "Par example l'API web de wikipedia est une instance de l'API wikimedia (le logiciel).\n",
    "\n",
    " - https://www.mediawiki.org/wiki/API:Main_page\n",
    "\n",
    "Les API ne sont pas limitez au web – nous en discuterons vers la fin du cours plus en details. \n",
    "Les ensembles de fonctions d'une librariries, leurs signatures et le types de données definisse un API. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API web\n",
    "\n",
    "Lorsque vous cherchez a acceder à des donnés sur le web, une des premières étapes est de chercher si le service a une API; Le code que vous allez écrire pour parler avec une API sera souvent plus simple, que de parser les donnés venant d'une page web.\n",
    "\n",
    "Utiliser une API permet souvent de faire plus de choses que d'interagir directement avec le site web.\n",
    "\n",
    "Contrairement aux site web où l'on va souvent parler de \"pages\", les API seront structurés en requetes et réponses, dont les paramètres sont strictes. \n",
    "\n",
    "Les réponse sont rarement prévues pour être directement lues par l'humain.\n",
    "\n",
    "Nous verrons aussi que les API sont souvent classés en categories, avec des acronymes ou non de technologies, (SOAP, RPC, RST, graphql)\n",
    "\n",
    "\n",
    "Tout comme pour les pages web, nous allons faire des requêtes http(s), cepandant les réponses seront prévu pour être lu par une machine. Un grand nombre de details que l'on ne vois pas quand on navigue le web peuvent devenir apparent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "b'[\"social\",[\"Social\",\"Socialism\",\"Social media\",\"Socialist Federal Republic of Yugoslavia\",\"Social democracy\",\"Societal collapse\",\"Sociology\",\"Social networking service\",\"Social issue\",\"Social science\"],[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"],[\"https://en.wikipedia.org/wiki/Social\",\"https://en.wikipedia.org/wiki/Socialism\",\"https://en.wikipedia.org/wiki/Social_media\",\"https://en.wikipedia.org/wiki/Socialist_Federal_Republic_of_Yugoslavia\",\"https://en.wikipedia.org/wiki/Social_democracy\",\"https://en.wikipedia.org/wiki/Societal_collapse\",\"https://en.wikipedia.org/wiki/Sociology\",\"https://en.wikipedia.org/wiki/Social_networking_service\",\"https://en.wikipedia.org/wiki/Social_issue\",\"https://en.wikipedia.org/wiki/Social_science\"]]'\n"
     ]
    }
   ],
   "source": [
    "# examples de requêtes et réponses\n",
    "\n",
    "import requests\n",
    "from requests import Request\n",
    "\n",
    "\n",
    "response = requests.get('https://en.wikipedia.org/w/api.php?action=opensearch&search=social&limit=10', )\n",
    "# https://en.wikipedia.org/w/api.php\n",
    "#   ?    (standard)\n",
    "#   action=opensearch\n",
    "#   &    \n",
    "#   search=social\n",
    "#   &\n",
    "#   limit=10\n",
    "print(response.status_code) # 200 all is fine\n",
    "print(response.content) # note it's bytes (actually json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    ">  Les paramètres dans l'URL sont visible par votre fournisseur d'access, et toute personnes entre vous et le serveur. Contrairement au header et body.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request?\n",
    "# Request.<tab>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On est content, wikipedia retourne du json, c'est \"facile\" a parsr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['social',\n",
       " ['Social',\n",
       "  'Socialism',\n",
       "  'Social media',\n",
       "  'Socialist Federal Republic of Yugoslavia',\n",
       "  'Social democracy',\n",
       "  'Societal collapse',\n",
       "  'Sociology',\n",
       "  'Social networking service',\n",
       "  'Social issue',\n",
       "  'Social science'],\n",
       " ['', '', '', '', '', '', '', '', '', ''],\n",
       " ['https://en.wikipedia.org/wiki/Social',\n",
       "  'https://en.wikipedia.org/wiki/Socialism',\n",
       "  'https://en.wikipedia.org/wiki/Social_media',\n",
       "  'https://en.wikipedia.org/wiki/Socialist_Federal_Republic_of_Yugoslavia',\n",
       "  'https://en.wikipedia.org/wiki/Social_democracy',\n",
       "  'https://en.wikipedia.org/wiki/Societal_collapse',\n",
       "  'https://en.wikipedia.org/wiki/Sociology',\n",
       "  'https://en.wikipedia.org/wiki/Social_networking_service',\n",
       "  'https://en.wikipedia.org/wiki/Social_issue',\n",
       "  'https://en.wikipedia.org/wiki/Social_science']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res = requests.get(\"https://en.wikipedia.org/w/api.php?\"\n",
    "    \"action=query\"\n",
    "    \"&prop=revisions\"\n",
    "    \"&titles=Sociology\"\n",
    "    \"&rvprop=content\"\n",
    "    \"&rvslots=main&formatversion=2\"\n",
    "    \"&format=json\")\n",
    "#res.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En pratique, on on va chercher des bibliothèques qui font ça pour Nous. De la même manière qu'on preferera `pd.read_csv(...)` au lieu de `with open(...) as f:...`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Using cached wikipedia-1.4.0-py3-none-any.whl\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.9.3-py3-none-any.whl (115 kB)\n",
      "\u001b[K     |████████████████████████████████| 115 kB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.0.0 in /Users/bussonniermatthias/miniconda3/lib/python3.8/site-packages (from wikipedia) (2.24.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/bussonniermatthias/miniconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/bussonniermatthias/miniconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/bussonniermatthias/miniconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/bussonniermatthias/miniconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.2.1-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, wikipedia\n",
      "Successfully installed beautifulsoup4-4.9.3 soupsieve-2.2.1 wikipedia-1.4.0\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/Users/bussonniermatthias/miniconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sociology',\n",
       " 'Lifestyle (sociology)',\n",
       " 'Dramaturgy (sociology)',\n",
       " 'Economic sociology',\n",
       " 'Positivism',\n",
       " 'Sociology of religion',\n",
       " 'Dyad (sociology)',\n",
       " 'Deviance (sociology)',\n",
       " 'Sociology of education',\n",
       " 'Sociology (disambiguation)']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = wikipedia.search('Sociology')\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de comencer sur l'utilisation de la bibliothèques wikipedia, notons que l'utilisation d'une API peut souvent être contraint à des limites légales. Il se peut que vous ayez a créer un compte developpeur; et indiquez la raison pour laquelle vous ustilisez un API. \n",
    "\n",
    "Certaines API sont \"officielles\" (wikipedia, github), d'autre pas (instagram, une partie de twitter).\n",
    "\n",
    "Les API sont aussi souvent limiter en nombre de requètes par heure; il est souvent facile de faire une boucle \"for\" et de dépasser la limite.\n",
    "\n",
    "Les API ne sont pas limité à récupérer des données, il est aussi possible de modifier/ajouter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "API : approche très computer science\n",
    "\n",
    "API, notion générale, et notion orientée web sur laquelle nous allons d'abord commencer dans ce cours.\n",
    "\n",
    "API et wrappers; Lien avec les bibliothèques\n",
    "\n",
    "-> S3\n",
    "\n",
    "Petit point sur plus généralement la manipulation de données : Sensibilisation aux limites légales etc.\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quelques éléments à savoir faire :\n",
    "\n",
    "- Rechercher, regarder la documentation, mettre en oeuvre et récupérer les données\n",
    "- Vérifier l'actualité de l'API (ex. de GetOldTweets)\n",
    "- Trois cas rapide : Wikipédia & Google Scholar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques exemples possibles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipédia\n",
    "\n",
    "https://pypi.org/project/wikipedia/\n",
    "\n",
    "Les différents éléments autour de Python\n",
    "\n",
    "Exemple du fait qu'elle a changé entre notre code et la publi du bouquin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notons que la biliothèque pythno `wikipedia` à elle même une \"API\" dans le sens général:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "wikipedia.wikipedia.page(\n",
    "    title=None,\n",
    "    pageid=None,\n",
    "    auto_suggest=True,\n",
    "    redirect=True,\n",
    "    preload=False,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si on renomais \"pageid\" en \"id\", l'API changerai."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'API de wikipedia a aussi changé en Automne 2020. \"search\" ne revoyais pas les pages de déambiguataion auparavent et le fait maintenant. Les pages de desambiguations n'ont pas de contenu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sociology is the study of society, human social behaviour, patterns of social relationships, social ...'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = wikipedia.page(res[0])\n",
    "page.content[:100]+'...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarquez que nous obtenons ici les données brutes sans l'expansion des templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia.set_lang(\"fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sociologie',\n",
       " 'Sociology',\n",
       " 'Interactionnisme structural',\n",
       " 'Sociologie des sciences',\n",
       " 'Histoire de la sociologie',\n",
       " 'Théorie sociologique',\n",
       " 'Cisgenre',\n",
       " 'Georg Simmel',\n",
       " 'La Construction sociale de la réalité',\n",
       " 'Contemporary Sociology']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia.search('Sociology')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = wikipedia.page('Lausanne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Decimal('48.60249999999999914734871708787977695465087890625'),\n",
       " Decimal('-2.824166669999999879081542530911974608898162841796875'))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En particulier en remarquera que les API nous retournes des donnés qui sont prévu pour ne pas être au format text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Géolocalisation avec OSM\n",
    "\n",
    "https://pypi.org/project/geocoder/0.5.7/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[48.6007887, -2.8248754]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geocoder\n",
    "g = geocoder.osm('Lausanne, Suisse')\n",
    "g.latlng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Scholar\n",
    "\n",
    "Par exemple pour faire de la scientométrie\n",
    "\n",
    "https://scholarly.readthedocs.io/en/latest/quickstart.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scholarly import scholarly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emilien Schultz\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the author's data, fill-in, and print\n",
    "search_query = scholarly.search_author('Émilien Schultz')\n",
    "author = scholarly.fill(next(search_query))\n",
    "print(author['name'])\n",
    "\n",
    "# Print the titles of the author's publications\n",
    "#print([pub['bib']['title'] for pub in author['publications']])\n",
    "\n",
    "# Take a closer look at the first publication\n",
    "#pub = scholarly.fill(author['publications'][0])\n",
    "#print(pub)\n",
    "\n",
    "# Which papers cited that publication?\n",
    "#print([citation['bib']['title'] for citation in scholarly.citedby(pub)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moment 2 - Utiliser une API plus complexe : Twitter (Emilien, 30 min)\n",
    "\n",
    "- Tweepy : https://github.com/tweepy/tweepy\n",
    "- Regarder la documentation\n",
    "- l'API Twitter, créer un compte et demander des crédentiels\n",
    "- Mettre ses crédentials\n",
    "- Collecter les tweets récents sur pyshs ?\n",
    "- Collecter les tweets autour d'islamogauchiste sur une période\n",
    "- Regarder les données et les mettre en forme\n",
    "- Créer un collecteur qui s'inscrit dans le temps..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faire un test avec le tutorial de tweepy : les deux étapes de configuration de l'API puis son utilisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open source maintainers be like \"Hey we'd like money to develop faster CPUs\" and businesses be line, \"no thank you… https://t.co/tZfdeleGS7\n",
      "RT @_msw_: We are hiring for the AWS SageMaker #OpenSource JupyterLab contribution team.\n",
      "\n",
      "This role will be focused on upstream development…\n",
      "RT @david_latouche: Basthon, est une interface en ligne, développée par Romain CASATI @BasthonPython, respectueuse de votre vie privée. Ell…\n",
      "RT @_rlg: Cette vidéo, fou rire du matin (envoyée par un doctorant du labo, du coup je me demande à quel couplet j'en suis...).  https://t.…\n",
      "RT @Nature: Switzerland’s largest government research-funding agency, @snsf_ch, has started using a random-selection process as a tiebreake…\n",
      "RT @JohnBlaxland1: Wow! A designer made a 'map of the internet' depicting 3,000 websites as countries in an online world https://t.co/dXyCH…\n",
      "Saturday night, can't connect to bank account. Open JS console. Refresh. ... seeing the logs I guess someone is deb… https://t.co/SbyQq7412W\n",
      "RT @isabelapf2: The next Jupyter community call is coming up on Tuesday, May 25th at 8am Pacific. Whether you've been in the community a lo…\n",
      "Préparation de la prochaine séance du cours #pyshs à Lausanne - les API. Test de l'API :)\n",
      "RT @neuro_rish: @Mbussonn @jeremyphoward @Zoom Jitsi has that feature, love it!\n",
      "ok, @Zoom idea: A pie chart of cumulative  speaker time to _gently_ remind some people that  they monopolize the co… https://t.co/jc2uktWpIo\n",
      "RT @afs_socio: Annonce importante : notre congrès, prévu du 6 au 9 juillet 2021 à Lille, se tiendra entièrement en distanciel. Plus de déta…\n",
      "Bon @UPS_FR il faudrait former vos livreurs au code de la route car à ma question s'il voyait le problème d'être st… https://t.co/IunTBaWwE9\n",
      "RT @ADRIPS_comm: #Doctorat\n",
      "#Financement\n",
      "\n",
      "Thèse (LaPEA, Versailles) : \"Modélisation de l'acceptabilité de mesures publiques et d'initiatives…\n",
      "RT @OndrejCertik: My thoughts on the fact that the next version of LAPACK is planned to be implemented in C++ instead of Fortran:\n",
      "\n",
      "https://…\n",
      "RT @Gometmedia: [Urgent] 🔴 🚴👏 Le 1er dimanche sans voitures à #Marseille aura lieu le 23 mai sur la #Corniche Un bon début... Roulez, march…\n",
      "RT @katyhuff: Exciting to be part of a slate of such phenomenal appointees announced officially this week @Energy! I was especially happy t…\n",
      "In case you missed it on friday, here is some thoughts and demo on how to improve Scientific Python ecosystem docum… https://t.co/BdT0NqWces\n",
      "RT @katyhuff: I’m thrilled to finally share that today is my 1st day in the Department of Energy's Office of Nuclear Energy (@Energy @GovNu…\n",
      "RT @102bis: Ces deux mois de silence sur Twitter, c’était le temps d’imaginer avec toute l’équipe démissionnaire de #scienceetvie le magazi…\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "\n",
    "consumer_key = \"mHqUaZkujRDAAn20gJJAHu3ly\"\n",
    "consumer_secret = \"Iz237BNqXCSStFetp196GwNBXWNTHmmJFit6ZcdxAwwHP17rf0\"\n",
    "access_token = \"1388816341516365825-uQKcLImidWmUhCpNoeVXtftbvDA6tO\"\n",
    "access_token_secret = \"m9ug3cTZIdMAY14ktwd0WXB8ms8HKdijmUMyFnOxWxtKJ\"\n",
    "\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "public_tweets = api.home_timeline()\n",
    "for tweet in public_tweets:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poster un tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = api.update_status(\"Préparation de la prochaine séance du cours #pyshs à Lausanne - les API. Test de l'API :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mettre le statut à jour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = api.update_profile(description=\"Réflexion collective autour de Python pour les Sciences Humaines et Sociales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etudier un utilisateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1388816341516365825"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user = api.get_user(\"pyshs1\")\n",
    "user.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récupérer tous les tweets mentionnant pyshs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récupérer tous les tweets mentionnant islamogauchisme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = api.search(q=\"islamogauchisme\",count=100,lang=\"fr\")\n",
    "len(corpus)\n",
    "#for tweet in corpus:\n",
    "#    print(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweepy.models.SearchResults"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [i for i in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EstelleNasty'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = corpus[1]\n",
    "t.user.screen_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2021, 5, 15, 12, 28, 20)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.created_at"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intégrer ensuite ces données dans un traitement : par exemple en faire un tableau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tableau = pd.DataFrame([[t.id_str,t.created_at,i.user.id,i.user.screen_name, i.text,i.user.location] for i in corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Orfeo_Giorgi      3\n",
       "_Attano_          3\n",
       "CestinEric        2\n",
       "divineFrance21    2\n",
       "CarminaRosa19     2\n",
       "                 ..\n",
       "RDD271999019      1\n",
       "lohoudominique    1\n",
       "DelatourRegis     1\n",
       "tonioaa1          1\n",
       "Adlost5           1\n",
       "Name: 3, Length: 91, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tableau[3].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention limite de 7 jours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Différents niveaux d'accès : API premium\n",
    "- https://developer.twitter.com/en/account/environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = api.search_full_archive(environment_name=\"training\",query=\"islamogauchisme\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pas si facile de se constituer un corpus ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Petite réflexion sur la sécurité des données avec des codes ? Rendre public ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "consumer_key = \"mHqUaZkujRDAAn20gJJAHu3ly\"\n",
    "consumer_secret = \"Iz237BNqXCSStFetp196GwNBXWNTHmmJFit6ZcdxAwwHP17rf0\"\n",
    "access_token = \"1388816341516365825-pQddB13qZxsz3DSHqeKMW8vmHAq2OY\"\n",
    "access_token_secret = \"mSIM8JQwJi3IU1F5MF9bLWRVQIlPzD3d1LHPjCfYu74L3\"\n",
    "\n",
    "codes = {\"consumer_key\":consumer_key,\"consumer_secret\":consumer_secret,\n",
    "         \"access_token\":access_token,\"access_token_secret\":access_token_secret}\n",
    "\n",
    "with open(\"twitter.keys\",\"w\") as f:\n",
    "    json.dump(codes,f)\n",
    "    \n",
    "with open(\"twitter.keys\",\"r\") as f:\n",
    "    codes = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créer une collecte en continue de tweets :\n",
    "- les stratégies ?\n",
    "- aller plus loin dans la compréhension de Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a donc plusieurs briques qui permettent de créer notre traitement de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Des usages plus avancés de l'API : collecter un flux de tweets\n",
    "\n",
    "- un scrit qui fait une veille permanente\n",
    "- doit être exécuté sur un ordinateur dans la durée (serveur ?)\n",
    "- des éléments des bibliothèques permettant de coder ce type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API initialisée\n",
      "1393549207861071875\n",
      "Erreur rencontrée\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "import json\n",
    "\n",
    "# StreamListener class inherits from tweepy.StreamListener and overrides on_status/on_error methods.\n",
    "class StreamListener(tweepy.StreamListener):\n",
    "    def on_status(self, status):\n",
    "        print(status.id_str)\n",
    "        # if \"retweeted_status\" attribute exists, flag this tweet as a retweet.\n",
    "        is_retweet = hasattr(status, \"retweeted_status\")\n",
    "\n",
    "        # check if text has been truncated\n",
    "        if hasattr(status,\"extended_tweet\"):\n",
    "            text = status.extended_tweet[\"full_text\"]\n",
    "        else:\n",
    "            text = status.text\n",
    "\n",
    "        # check if this is a quote tweet.\n",
    "        is_quote = hasattr(status, \"quoted_status\")\n",
    "        quoted_text = \"\"\n",
    "        if is_quote:\n",
    "            # check if quoted tweet's text has been truncated before recording it\n",
    "            if hasattr(status.quoted_status,\"extended_tweet\"):\n",
    "                quoted_text = status.quoted_status.extended_tweet[\"full_text\"]\n",
    "            else:\n",
    "                quoted_text = status.quoted_status.text\n",
    "\n",
    "        # remove characters that might cause problems with csv encoding\n",
    "        remove_characters = [\",\",\"\\n\"]\n",
    "        for c in remove_characters:\n",
    "            text.replace(c,\" \")\n",
    "            quoted_text.replace(c, \" \")\n",
    "            \n",
    "        with open(\"tweets/\"+status.id_str,\"w\") as f:\n",
    "            json.dump(status._json,f)\n",
    "\n",
    "    def on_error(self, status_code):\n",
    "        print(\"Encountered streaming error (\", status_code, \")\")\n",
    "\n",
    "\n",
    "def launch():\n",
    "    try:\n",
    "        auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "        auth.set_access_token(access_token, access_token_secret)\n",
    "        api = tweepy.API(auth)\n",
    "        print(\"API initialisée\")\n",
    "        streamListener = StreamListener()\n",
    "        stream = tweepy.Stream(auth=api.auth, listener=streamListener,tweet_mode='extended')\n",
    "        tags = [\"islamogauchisme\"]\n",
    "        stream.filter(track=tags)\n",
    "    except:\n",
    "        print(\"Erreur rencontrée\")\n",
    "        #launch()\n",
    "    \n",
    "launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les limites de Tweepy, et la nécessité de passer par l'API Twitter\n",
    "- https://developer.twitter.com/en/docs/twitter-api/tweets/search/introduction\n",
    "- https://github.com/twitterdev/search-tweets-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moment 3 - Pas d'API : collecter directement des données sur internet (Emilien, 30 min)\n",
    "\n",
    "- Dans certains cas il n'y a pas d'API disponible, récupérer directement les données par les interfaces \"standard\"\n",
    "- Utiliser requests et BeautifulSoup\n",
    "- Importance de la rétro-ingénieurie : comprendre l'architecture d'une page web\n",
    "- Différentes stratégies : regex ou bibliothèques plus avancées\n",
    "- Récupérer des images ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récupérer des notices de livres Python sur Wordcat https://www.worldcat.org/ puis mettre en forme dans un fichier\n",
    "- chercher islamo-gauchisme\n",
    "- voir la forme de l'URL https://www.worldcat.org/search?q=islamo-gauchisme&fq=&dblist=638&start=21&qt=page_number_link\n",
    "\n",
    "Démarche : \n",
    "- récupérer les pages de résultat\n",
    "- construire un tableau des liens vers les résultats\n",
    "- récupérer chaque notice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Première étape regarder un peu les éléments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupérer une page avec requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://www.worldcat.org/search?q=islamo-gauchisme&fq=&dblist=638&start=1&qt=page_number_link\"\n",
    "\n",
    "page = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toutes les pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.worldcat.org/search?q=islamo-gauchisme&fq=&dblist=638&start=1&qt=page_number_link\n",
      "https://www.worldcat.org/search?q=islamo-gauchisme&fq=&dblist=638&start=11&qt=page_number_link\n",
      "https://www.worldcat.org/search?q=islamo-gauchisme&fq=&dblist=638&start=21&qt=page_number_link\n",
      "https://www.worldcat.org/search?q=islamo-gauchisme&fq=&dblist=638&start=31&qt=page_number_link\n",
      "https://www.worldcat.org/search?q=islamo-gauchisme&fq=&dblist=638&start=41&qt=page_number_link\n",
      "https://www.worldcat.org/search?q=islamo-gauchisme&fq=&dblist=638&start=51&qt=page_number_link\n",
      "https://www.worldcat.org/search?q=islamo-gauchisme&fq=&dblist=638&start=61&qt=page_number_link\n"
     ]
    }
   ],
   "source": [
    "pages = []\n",
    "for i in range(0,7):\n",
    "    url = \"https://www.worldcat.org/search?q=islamo-gauchisme&fq=&dblist=638&start={}&qt=page_number_link\".format(1+10*i)\n",
    "    print(url)\n",
    "    page = requests.get(url)\n",
    "    pages.append(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraire les liens : un peu de rétroingénierie\n",
    "\n",
    "Plusieurs stratégies :\n",
    "- tous les liens et filtrer\n",
    "- respecter la structure du document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se balader dans la structure html : BeautifoulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = bs4.BeautifulSoup(pages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "liens = page.find_all(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "liens = [i.attrs[\"href\"] for i in liens if \"href\" in i.attrs and \"/oclc/\" in i.attrs[\"href\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/title/allons-nous-sortir-de-lhistoire/oclc/1100451295',\n",
       " '/title/emirats-de-la-republique-comment-les-islamistes-prennent-possession-de-la-banlieue/oclc/1140404936',\n",
       " '/title/empoisonneurs-antisemitisme-islamophobie-xenophobie/oclc/1198222296',\n",
       " '/title/islamo-gauchisme/oclc/8760612072',\n",
       " '/title/islamo-gauchisme/oclc/8932537540',\n",
       " '/title/liaisons-dangereuses-islamo-nazisme-islamo-gauchisme/oclc/1248694509',\n",
       " '/title/livre-des-indesires-une-histoire-des-arabes-en-france/oclc/1084514701',\n",
       " '/title/negationnisme-de-gauche/oclc/1099928429',\n",
       " '/title/racines-de-lislamo-gauchisme-dossier/oclc/1057453730',\n",
       " '/title/racisme-imaginaire-islamophobie-et-culpabilite/oclc/974816801'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([i.replace(\"&referer=brief_results\",\"\").replace(\"/editions?editionsView=true&referer=br\",\"\") for i in liens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = liens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.attrs[\"href\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ecrire une fonction pour récupérer les éléments dont on a besoin sur chacune des pages ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.worldcat.org/title/islamo-gauchisme-du-pseudo-debat-a-la-realite-du-terrain/oclc/8999871764\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = bs4.BeautifulSoup(requests.get(url).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "details = page.find_all(\"div\",{\"id\":\"details\"})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = details.find_all(\"div\",{\"class\":\"abstracttxt\"})[0].text\n",
    "doctype =  details.find_all(\"tr\",{\"id\":\"details-doctype\"})[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(url):\n",
    "    page = bs4.BeautifulSoup(requests.get(url).content)\n",
    "    details = page.find_all(\"div\",{\"id\":\"details\"})[0]\n",
    "    resume = details.find_all(\"div\",{\"class\":\"abstracttxt\"})[0].text\n",
    "    doctype =  details.find_all(\"tr\",{\"id\":\"details-doctype\"})[0].text\n",
    "    return [url,resume,doctype]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.worldcat.org/title/islamo-gauchisme-du-pseudo-debat-a-la-realite-du-terrain/oclc/8999871764',\n",
       " \"\\n           Au cœur de ce débat qui prend des tournures toujours plus inattendues, j'ai eu la chance d'avoir une conversation exigeante et mesurée, afin de prendre un peu de recul. Merci à CTRL Z pour sa confiance et, au passage, pour ses articles d'une trop rare finesse sur le mondes numériques et les débats de notre temps ! La conversation menée avec la mirifique Elodie Safaris est accessible sur Youtube ou Spotify ; je vous incruste ici la version Youtube. https://www.youtube.com/embed/ausj14hU-e...\",\n",
       " '\\nDocument Type:\\nArticle\\n']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://www.worldcat.org/title/islamo-gauchisme-du-pseudo-debat-a-la-realite-du-terrain/oclc/8999871764\"\n",
    "get_info(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tout intégrer dans un script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(0,7):\n",
    "    url = \"https://www.worldcat.org/search?q=islamo-gauchisme&fq=&dblist=638&start={}&qt=page_number_link\".format(1+10*i)\n",
    "    page = requests.get(url)\n",
    "    page = bs4.BeautifulSoup(page.content)\n",
    "    liens = page.find_all(\"a\")\n",
    "    liens = [i.attrs[\"href\"] for i in liens if \"href\" in i.attrs and \"/oclc/\" in i.attrs[\"href\"]]\n",
    "    liens = set([i.replace(\"&referer=brief_results\",\"\").replace(\"/editions?editionsView=true&referer=br\",\"\") for i in liens])\n",
    "    corpus+=list(liens)\n",
    "\n",
    "data = []\n",
    "for url in corpus:\n",
    "    data.append(get_info(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moment 4 - Plus d'API, du point de vue humain à celui de l'ordinateur (Matthias, 15-30 min)\n",
    "\n",
    "Différents types d'API / Généralisation\n",
    "\n",
    "\n",
    "Les biliothèques peuvent avoir un notion d'API.\n",
    "\n",
    " - Par exemple Vaex, et Dask-dataframe sont deux bibliothèques qui expose une API similaire a Pandas, inndiquant \n",
    "\n",
    "Différentes version d'API\n",
    "\n",
    " - les bibliothèques vont souvent essayer de garder un API similaire pour des version proche. En général une bibliothèque version X.y.z and X.a.b vont avoir des API \"compatible\".\n",
    "\n",
    " - Les API web parfois on la possibilité de choisir la version de l'API avec laquelle on interagit.\n",
    "\n",
    "usages plus compliqués : async/await\n",
    "\n",
    " - Pour la collection massive de données, cherchez async/await qui permet de récuperer des résultats de manière concourante. \n",
    " - Rate limits\n",
    "Créer sa propre API ? Garder la signature des fonctions / https://fastapi.tiangolo.com/\n",
    "\n",
    "\n",
    "\n",
    "S3 / Lien avec les bases de données ? Exemple de sqlite ? https://docs.python.org/3/library/sqlite3.html \n",
    "\n",
    "Quelques liens\n",
    "\n",
    "- Livres Gallica : https://api.bnf.fr/fr/wrapper-python-pour-les-api-gallica / https://github.com/ian-nai/PyGallica\n",
    "- Vidéos : https://pypi.org/project/python-youtube/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quelques Termes  quand on parle d'API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Pas besoin de retenir, mais au cas où). \n",
    "\n",
    "REST est un type d'API assez courant, utilisé pour modifier des \"documents\" – par example, tweets, page wikipedia.\n",
    "  - GET (requests.get) pour obtenir un document \n",
    "  - POST (request.post) générique en parculier si il y a des informations qui ne sont pas dans l'URL à envoyer au serveur.\n",
    "  - PUT (request.put) pour téléverser un document\n",
    "  - PATCH (request.patch) pour modifier un document\n",
    "  - HEAD (request.head) recevoir  uniquement les metadonnées\n",
    "  - DELETE (request.delete)\n",
    "  \n",
    "Dans la documentation on Anglais il peut être difficile de repérer que ces termes sont des termes techniques.\n",
    "\n",
    "\n",
    "Status Code:\n",
    "  - 2xx : Tout vas bien\n",
    "  - 3xx (souvent 304) : Problème d'autentification\n",
    "  - 4xx : ça existe pas.\n",
    "  - 5xx : données incorrect ou le serveur a planté.\n",
    "  \n",
    "HTTP(s):\n",
    "\n",
    "  - body and header. (encryted) \n",
    "     - header = Metadata\n",
    "     - body = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date                           Thu, 20 May 2021 05:23:36 GMT\n",
      "Server                         mw1406.eqiad.wmnet\n",
      "X-Content-Type-Options         nosniff\n",
      "P3p                            CP=\"See https://en.wikipedia.org/wiki/Special:CentralAutoLogin/P3P for more info.\"\n",
      "X-Search-Id                    39vc56oobudouvqoqe48u5tyt\n",
      "X-Opensearch-Type              comp_suggest\n",
      "X-Frame-Options                SAMEORIGIN\n",
      "Content-Disposition            inline; filename=api-result.json\n",
      "Vary                           Accept-Encoding,Treat-as-Untrusted,X-Forwarded-Proto,Cookie,Authorization\n",
      "Expires                        Thu, 20 May 2021 08:23:36 GMT\n",
      "Cache-Control                  max-age=10800, s-maxage=10800, public\n",
      "X-Request-Id                   d6b4e1be-2c60-4568-bb2d-07d9b429400f\n",
      "Content-Type                   application/json; charset=utf-8\n",
      "Content-Encoding               gzip\n",
      "Age                            19\n",
      "X-Cache                        cp4030 miss, cp4029 hit/1\n",
      "X-Cache-Status                 hit-front\n",
      "Server-Timing                  cache;desc=\"hit-front\", host;desc=\"cp4029\"\n",
      "Strict-Transport-Security      max-age=106384710; includeSubDomains; preload\n",
      "Report-To                      { \"group\": \"wm_nel\", \"max_age\": 86400, \"endpoints\": [{ \"url\": \"https://intake-logging.wikimedia.org/v1/events?stream=w3c.reportingapi.network_error&schema_uri=/w3c/reportingapi/network_error/1.0.0\" }] }\n",
      "NEL                            { \"report_to\": \"wm_nel\", \"max_age\": 86400, \"failure_fraction\": 0.05, \"success_fraction\": 0.0}\n",
      "Permissions-Policy             interest-cohort=()\n",
      "Set-Cookie                     WMF-Last-Access=20-May-2021;Path=/;HttpOnly;secure;Expires=Mon, 21 Jun 2021 00:00:00 GMT, WMF-Last-Access-Global=20-May-2021;Path=/;Domain=.wikipedia.org;HttpOnly;secure;Expires=Mon, 21 Jun 2021 00:00:00 GMT, GeoIP=US:CA:Merced:37.33:-120.50:v4; Path=/; secure; Domain=.wikipedia.org\n",
      "X-Client-IP                    73.90.141.68\n",
      "Accept-Ranges                  bytes\n",
      "Content-Length                 226\n",
      "Connection                     keep-alive\n"
     ]
    }
   ],
   "source": [
    "for k,v in response.headers.items():\n",
    "    print(f\"{k:<30}\", v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      \n",
    " \n",
    "\n",
    "Reseaux-Sociaux:\n",
    "\n",
    "  - GraphQL: Renvoie uniquement les elements demandés + récursif. Tout ce qui est difficile en tabulaire.\n",
    "    \"@pyshs1 -> Tweet avec le plus de like -> utilisateur qui on liké ce tweet -> nom d'utilisateur\".\n",
    "      \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "mimetype": "text/python",
   "name": "Python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
