{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S√©ance 5 - Collecte de donn√©es et API\n",
    "\n",
    "L'objectif de la s√©ance est de voir un peu de la collecte de donn√©es sur internet, et pour cela de commencer √† se familiariser avec la notion d'API.\n",
    "\n",
    "L'id√©e √©tant que beaucoup de donn√©es existantes doivent √™tre acquises en s'interfa√ßant avec diff√©rents syst√®mes : serveurs, bases de donn√©es, etc. Tout cela conduit √† devoir ma√Ætriser diff√©rentes formes d'API\n",
    "\n",
    "Quatre moments :\n",
    "- Ce qu'est une API\n",
    "- Une API avec cl√© d'acc√®s : le cas de Twitter\n",
    "- Traiter des donn√©es du web sans API\n",
    "- Quelques notions plus avanc√©es sur les API\n",
    "\n",
    "https://medium.com/@perrysetgo/what-exactly-is-an-api-69f36968a41f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moment 1 - Ce qu'est une API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est quoi une API web, les donn√©es sur internet, etc.\n",
    "\n",
    "Une API (interface de programmation d‚Äôapplication) est un ensemble de conventions explicite ou implicite permettant a un ou des ordinateurs/programmes de communiquer. \n",
    "\n",
    "Ces conventions sont importantes afin de developper en isolation relative, et/ou de r√©utilser/changer des biblioth√®ques. Ce qui fait (ou ne fait pas partie) d'une API peut parfois sembler subjectif.\n",
    "\n",
    "Example \"d'API\" dans la vie de tous les jours.\n",
    " - Boulons/√âcroux : Le filetage est standard\n",
    " - Container\n",
    " - Code Barre.\n",
    " - 220V \n",
    " - On conduit a droite (du moins en europe)\n",
    " - Hocher la t√™te pour dire \"oui\"\n",
    " - Vert gen√©ralement Positif, Rouge g√©n√©ralement N√©gatif\n",
    " - Les pi√®ces de monaies.\n",
    " - ...\n",
    " \n",
    " \n",
    "Bien qu'aucune des conventions si-dessus soit n√©c√©ssaire pour une soci√©t√© bien huil√©e, il est g√©n√©ralement pr√©f√©rable de suivre ces conventions. Quelles conventions sont suivies peut parvois d√©pendre du context, et des personnes avec qui on interagis.\n",
    "\n",
    "Une API en programation est similaire dans le sens ou :\n",
    "  - La d√©finition exacte de API va d√©pendre du context ou l'on se place avec qui o√π quoi on communique\n",
    "      - Aux US la notion de se \"faire la bise\" est une notion boolean\n",
    "      - En Europe, √ßa fait √™tre combien, par quel cot√© en commence.\n",
    "  - Tant que possible change peu\n",
    "  \n",
    "Cependant un API en programation was souvent avoir beaucoup plus de details dans ses valeurs, et va √™tre plus stricte; √©tant pr√©vu majoritairement pour communication machine <-> machine, (avec parfois le programmeur), la verbosit√© n'est pas un probl√®me.\n",
    "\n",
    "Une API va souvent aussi contenir les donn√©s brutes, par example \"2020-05-09T13:49:54+00:00\" au lieu de \"9 May 2020\".\n",
    "\n",
    "Les documentations D'API sont pratiquement toujours tr√®s abstraites, et la documentation va √™tre assez g√©n√©rique.\n",
    "\n",
    "Par example l'API web de wikipedia est une instance de l'API wikimedia (le logiciel).\n",
    "\n",
    " - https://www.mediawiki.org/wiki/API:Main_page\n",
    "\n",
    "Les API ne sont pas limitez au web ‚Äì nous en discuterons vers la fin du cours plus en details. \n",
    "Les ensembles de fonctions d'une librariries, leurs signatures et le types de donn√©es definisse un API. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API web\n",
    "\n",
    "Lorsque vous cherchez a acceder √† des donn√©s sur le web, une des premi√®res √©tapes est de chercher si le service a une API; Le code que vous allez √©crire pour parler avec une API sera souvent plus simple, que de parser les donn√©s venant d'une page web.\n",
    "\n",
    "Utiliser une API permet souvent de faire plus de choses que d'interagir directement avec le site web.\n",
    "\n",
    "Contrairement aux site web o√π l'on va souvent parler de \"pages\", les API seront structur√©s en requetes et r√©ponses, dont les param√®tres sont strictes. \n",
    "\n",
    "Les r√©ponse sont rarement pr√©vues pour √™tre directement lues par l'humain.\n",
    "\n",
    "Nous verrons aussi que les API sont souvent class√©s en categories, avec des acronymes ou non de technologies, (SOAP, RPC, RST, graphql)\n",
    "\n",
    "\n",
    "Tout comme pour les pages web, nous allons faire des requ√™tes http(s), cepandant les r√©ponses seront pr√©vu pour √™tre lu par une machine. Un grand nombre de details que l'on ne vois pas quand on navigue le web peuvent devenir apparent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "b'[\"social\",[\"Social\",\"Socialism\",\"Social media\",\"Socialist Federal Republic of Yugoslavia\",\"Social democracy\",\"Societal collapse\",\"Sociology\",\"Social networking service\",\"Social issue\",\"Social science\"],[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"],[\"https://en.wikipedia.org/wiki/Social\",\"https://en.wikipedia.org/wiki/Socialism\",\"https://en.wikipedia.org/wiki/Social_media\",\"https://en.wikipedia.org/wiki/Socialist_Federal_Republic_of_Yugoslavia\",\"https://en.wikipedia.org/wiki/Social_democracy\",\"https://en.wikipedia.org/wiki/Societal_collapse\",\"https://en.wikipedia.org/wiki/Sociology\",\"https://en.wikipedia.org/wiki/Social_networking_service\",\"https://en.wikipedia.org/wiki/Social_issue\",\"https://en.wikipedia.org/wiki/Social_science\"]]'\n"
     ]
    }
   ],
   "source": [
    "# examples de requ√™tes et r√©ponses\n",
    "\n",
    "import requests\n",
    "from requests import Request\n",
    "\n",
    "\n",
    "response = requests.get('https://en.wikipedia.org/w/api.php?action=opensearch&search=social&limit=10', )\n",
    "# https://en.wikipedia.org/w/api.php\n",
    "#   ?    (standard)\n",
    "#   action=opensearch\n",
    "#   &    \n",
    "#   search=social\n",
    "#   &\n",
    "#   limit=10\n",
    "print(response.status_code) # 200 all is fine\n",
    "print(response.content) # note it's bytes (actually json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    ">  Les param√®tres dans l'URL sont visible par votre fournisseur d'access, et toute personnes entre vous et le serveur. Contrairement au header et body.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request?\n",
    "# Request.<tab>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On est content, wikipedia retourne du json, c'est \"facile\" a parsr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['social',\n",
       " ['Social',\n",
       "  'Socialism',\n",
       "  'Social media',\n",
       "  'Socialist Federal Republic of Yugoslavia',\n",
       "  'Social democracy',\n",
       "  'Societal collapse',\n",
       "  'Sociology',\n",
       "  'Social networking service',\n",
       "  'Social issue',\n",
       "  'Social science'],\n",
       " ['', '', '', '', '', '', '', '', '', ''],\n",
       " ['https://en.wikipedia.org/wiki/Social',\n",
       "  'https://en.wikipedia.org/wiki/Socialism',\n",
       "  'https://en.wikipedia.org/wiki/Social_media',\n",
       "  'https://en.wikipedia.org/wiki/Socialist_Federal_Republic_of_Yugoslavia',\n",
       "  'https://en.wikipedia.org/wiki/Social_democracy',\n",
       "  'https://en.wikipedia.org/wiki/Societal_collapse',\n",
       "  'https://en.wikipedia.org/wiki/Sociology',\n",
       "  'https://en.wikipedia.org/wiki/Social_networking_service',\n",
       "  'https://en.wikipedia.org/wiki/Social_issue',\n",
       "  'https://en.wikipedia.org/wiki/Social_science']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res = requests.get(\"https://en.wikipedia.org/w/api.php?\"\n",
    "    \"action=query\"\n",
    "    \"&prop=revisions\"\n",
    "    \"&titles=Sociology\"\n",
    "    \"&rvprop=content\"\n",
    "    \"&rvslots=main&formatversion=2\"\n",
    "    \"&format=json\")\n",
    "#res.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En pratique, on on va chercher des biblioth√®ques qui font √ßa pour Nous. De la m√™me mani√®re qu'on preferera `pd.read_csv(...)` au lieu de `with open(...) as f:...`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Using cached wikipedia-1.4.0-py3-none-any.whl\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.9.3-py3-none-any.whl (115 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 115 kB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.0.0 in /Users/bussonniermatthias/miniconda3/lib/python3.8/site-packages (from wikipedia) (2.24.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/bussonniermatthias/miniconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/bussonniermatthias/miniconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/bussonniermatthias/miniconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/bussonniermatthias/miniconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.2.1-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, wikipedia\n",
      "Successfully installed beautifulsoup4-4.9.3 soupsieve-2.2.1 wikipedia-1.4.0\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/Users/bussonniermatthias/miniconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sociology',\n",
       " 'Lifestyle (sociology)',\n",
       " 'Dramaturgy (sociology)',\n",
       " 'Economic sociology',\n",
       " 'Positivism',\n",
       " 'Sociology of religion',\n",
       " 'Dyad (sociology)',\n",
       " 'Deviance (sociology)',\n",
       " 'Sociology of education',\n",
       " 'Sociology (disambiguation)']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = wikipedia.search('Sociology')\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de comencer sur l'utilisation de la biblioth√®ques wikipedia, notons que l'utilisation d'une API peut souvent √™tre contraint √† des limites l√©gales. Il se peut que vous ayez a cr√©er un compte developpeur; et indiquez la raison pour laquelle vous ustilisez un API. \n",
    "\n",
    "Certaines API sont \"officielles\" (wikipedia, github), d'autre pas (instagram, une partie de twitter).\n",
    "\n",
    "Les API sont aussi souvent limiter en nombre de requ√®tes par heure; il est souvent facile de faire une boucle \"for\" et de d√©passer la limite.\n",
    "\n",
    "Les API ne sont pas limit√© √† r√©cup√©rer des donn√©es, il est aussi possible de modifier/ajouter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "API : approche tr√®s computer science\n",
    "\n",
    "API, notion g√©n√©rale, et notion orient√©e web sur laquelle nous allons d'abord commencer dans ce cours.\n",
    "\n",
    "API et wrappers; Lien avec les biblioth√®ques\n",
    "\n",
    "-> S3\n",
    "\n",
    "Petit point sur plus g√©n√©ralement la manipulation de donn√©es : Sensibilisation aux limites l√©gales etc.\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quelques √©l√©ments √† savoir faire :\n",
    "\n",
    "- Rechercher, regarder la documentation, mettre en oeuvre et r√©cup√©rer les donn√©es\n",
    "- V√©rifier l'actualit√© de l'API (ex. de GetOldTweets)\n",
    "- Trois cas rapide : Wikip√©dia & Google Scholar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques exemples possibles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikip√©dia\n",
    "\n",
    "https://pypi.org/project/wikipedia/\n",
    "\n",
    "Les diff√©rents √©l√©ments autour de Python\n",
    "\n",
    "Exemple du fait qu'elle a chang√© entre notre code et la publi du bouquin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notons que la bilioth√®que pythno `wikipedia` √† elle m√™me une \"API\" dans le sens g√©n√©ral:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "wikipedia.wikipedia.page(\n",
    "    title=None,\n",
    "    pageid=None,\n",
    "    auto_suggest=True,\n",
    "    redirect=True,\n",
    "    preload=False,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si on renomais \"pageid\" en \"id\", l'API changerai."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'API de wikipedia a aussi chang√© en Automne 2020. \"search\" ne revoyais pas les pages de d√©ambiguataion auparavent et le fait maintenant. Les pages de desambiguations n'ont pas de contenu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sociology is the study of society, human social behaviour, patterns of social relationships, social ...'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = wikipedia.page(res[0])\n",
    "page.content[:100]+'...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarquez que nous obtenons ici les donn√©es brutes sans l'expansion des templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia.set_lang(\"fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sociologie',\n",
       " 'Sociology',\n",
       " 'Interactionnisme structural',\n",
       " 'Sociologie des sciences',\n",
       " 'Histoire de la sociologie',\n",
       " 'Th√©orie sociologique',\n",
       " 'Cisgenre',\n",
       " 'Georg Simmel',\n",
       " 'La Construction sociale de la r√©alit√©',\n",
       " 'Contemporary Sociology']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia.search('Sociology')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = wikipedia.page('Lausanne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Decimal('48.60249999999999914734871708787977695465087890625'),\n",
       " Decimal('-2.824166669999999879081542530911974608898162841796875'))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En particulier en remarquera que les API nous retournes des donn√©s qui sont pr√©vu pour ne pas √™tre au format text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G√©olocalisation avec OSM\n",
    "\n",
    "https://pypi.org/project/geocoder/0.5.7/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[48.6007887, -2.8248754]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geocoder\n",
    "g = geocoder.osm('Lausanne, Suisse')\n",
    "g.latlng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Scholar\n",
    "\n",
    "Par exemple pour faire de la scientom√©trie\n",
    "\n",
    "https://scholarly.readthedocs.io/en/latest/quickstart.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scholarly import scholarly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emilien Schultz\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the author's data, fill-in, and print\n",
    "search_query = scholarly.search_author('√âmilien Schultz')\n",
    "author = scholarly.fill(next(search_query))\n",
    "print(author['name'])\n",
    "\n",
    "# Print the titles of the author's publications\n",
    "#print([pub['bib']['title'] for pub in author['publications']])\n",
    "\n",
    "# Take a closer look at the first publication\n",
    "#pub = scholarly.fill(author['publications'][0])\n",
    "#print(pub)\n",
    "\n",
    "# Which papers cited that publication?\n",
    "#print([citation['bib']['title'] for citation in scholarly.citedby(pub)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moment 2 - Utiliser une API plus complexe : Twitter (Emilien, 30 min)\n",
    "\n",
    "- Tweepy : https://github.com/tweepy/tweepy\n",
    "- Regarder la documentation\n",
    "- l'API Twitter, cr√©er un compte et demander des cr√©dentiels\n",
    "- Mettre ses cr√©dentials\n",
    "- Collecter les tweets r√©cents sur pyshs ?\n",
    "- Collecter les tweets autour d'islamogauchiste sur une p√©riode\n",
    "- Regarder les donn√©es et les mettre en forme\n",
    "- Cr√©er un collecteur qui s'inscrit dans le temps..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faire un test avec le tutorial de tweepy : les deux √©tapes de configuration de l'API puis son utilisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open source maintainers be like \"Hey we'd like money to develop faster CPUs\" and businesses be line, \"no thank you‚Ä¶ https://t.co/tZfdeleGS7\n",
      "RT @_msw_: We are hiring for the AWS SageMaker #OpenSource JupyterLab contribution team.\n",
      "\n",
      "This role will be focused on upstream development‚Ä¶\n",
      "RT @david_latouche: Basthon, est une interface en ligne, d√©velopp√©e par Romain CASATI @BasthonPython, respectueuse de votre vie priv√©e. Ell‚Ä¶\n",
      "RT @_rlg: Cette vid√©o, fou rire du matin (envoy√©e par un doctorant du labo, du coup je me demande √† quel couplet j'en suis...).  https://t.‚Ä¶\n",
      "RT @Nature: Switzerland‚Äôs largest government research-funding agency, @snsf_ch, has started using a random-selection process as a tiebreake‚Ä¶\n",
      "RT @JohnBlaxland1: Wow! A designer made a 'map of the internet' depicting 3,000 websites as countries in an online world https://t.co/dXyCH‚Ä¶\n",
      "Saturday night, can't connect to bank account. Open JS console. Refresh. ... seeing the logs I guess someone is deb‚Ä¶ https://t.co/SbyQq7412W\n",
      "RT @isabelapf2: The next Jupyter community call is coming up on Tuesday, May 25th at 8am Pacific. Whether you've been in the community a lo‚Ä¶\n",
      "Pr√©paration de la prochaine s√©ance du cours #pyshs √† Lausanne - les API. Test de l'API :)\n",
      "RT @neuro_rish: @Mbussonn @jeremyphoward @Zoom Jitsi has that feature, love it!\n",
      "ok, @Zoom idea: A pie chart of cumulative  speaker time to _gently_ remind some people that  they monopolize the co‚Ä¶ https://t.co/jc2uktWpIo\n",
      "RT @afs_socio: Annonce importante : notre congr√®s, pr√©vu du 6 au 9 juillet 2021 √† Lille, se tiendra enti√®rement en distanciel. Plus de d√©ta‚Ä¶\n",
      "Bon @UPS_FR il faudrait former vos livreurs au code de la route car √† ma question s'il voyait le probl√®me d'√™tre st‚Ä¶ https://t.co/IunTBaWwE9\n",
      "RT @ADRIPS_comm: #Doctorat\n",
      "#Financement\n",
      "\n",
      "Th√®se (LaPEA, Versailles) : \"Mod√©lisation de l'acceptabilit√© de mesures publiques et d'initiatives‚Ä¶\n",
      "RT @OndrejCertik: My thoughts on the fact that the next version of LAPACK is planned to be implemented in C++ instead of Fortran:\n",
      "\n",
      "https://‚Ä¶\n",
      "RT @Gometmedia: [Urgent] üî¥ üö¥üëè Le 1er dimanche sans voitures √† #Marseille aura lieu le 23 mai sur la #Corniche Un bon d√©but... Roulez, march‚Ä¶\n",
      "RT @katyhuff: Exciting to be part of a slate of such phenomenal appointees announced officially this week @Energy! I was especially happy t‚Ä¶\n",
      "In case you missed it on friday, here is some thoughts and demo on how to improve Scientific Python ecosystem docum‚Ä¶ https://t.co/BdT0NqWces\n",
      "RT @katyhuff: I‚Äôm thrilled to finally share that today is my 1st day in the Department of Energy's Office of Nuclear Energy (@Energy @GovNu‚Ä¶\n",
      "RT @102bis: Ces deux mois de silence sur Twitter, c‚Äô√©tait le temps d‚Äôimaginer avec toute l‚Äô√©quipe d√©missionnaire de #scienceetvie le magazi‚Ä¶\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "\n",
    "consumer_key = \"mHqUaZkujRDAAn20gJJAHu3ly\"\n",
    "consumer_secret = \"Iz237BNqXCSStFetp196GwNBXWNTHmmJFit6ZcdxAwwHP17rf0\"\n",
    "access_token = \"1388816341516365825-uQKcLImidWmUhCpNoeVXtftbvDA6tO\"\n",
    "access_token_secret = \"m9ug3cTZIdMAY14ktwd0WXB8ms8HKdijmUMyFnOxWxtKJ\"\n",
    "\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "public_tweets = api.home_timeline()\n",
    "for tweet in public_tweets:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poster un tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = api.update_status(\"Pr√©paration de la prochaine s√©ance du cours #pyshs √† Lausanne - les API. Test de l'API :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mettre le statut √† jour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = api.update_profile(description=\"R√©flexion collective autour de Python pour les Sciences Humaines et Sociales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etudier un utilisateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1388816341516365825"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user = api.get_user(\"pyshs1\")\n",
    "user.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R√©cup√©rer tous les tweets mentionnant pyshs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R√©cup√©rer tous les tweets mentionnant islamogauchisme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = api.search(q=\"islamogauchisme\",count=100,lang=\"fr\")\n",
    "len(corpus)\n",
    "#for tweet in corpus:\n",
    "#    print(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweepy.models.SearchResults"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [i for i in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EstelleNasty'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = corpus[1]\n",
    "t.user.screen_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2021, 5, 15, 12, 28, 20)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.created_at"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Int√©grer ensuite ces donn√©es dans un traitement : par exemple en faire un tableau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tableau = pd.DataFrame([[t.id_str,t.created_at,i.user.id,i.user.screen_name, i.text,i.user.location] for i in corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Orfeo_Giorgi      3\n",
       "_Attano_          3\n",
       "CestinEric        2\n",
       "divineFrance21    2\n",
       "CarminaRosa19     2\n",
       "                 ..\n",
       "RDD271999019      1\n",
       "lohoudominique    1\n",
       "DelatourRegis     1\n",
       "tonioaa1          1\n",
       "Adlost5           1\n",
       "Name: 3, Length: 91, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tableau[3].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention limite de 7 jours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diff√©rents niveaux d'acc√®s : API premium\n",
    "- https://developer.twitter.com/en/account/environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = api.search_full_archive(environment_name=\"training\",query=\"islamogauchisme\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pas si facile de se constituer un corpus ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Petite r√©flexion sur la s√©curit√© des donn√©es avec des codes ? Rendre public ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "consumer_key = \"mHqUaZkujRDAAn20gJJAHu3ly\"\n",
    "consumer_secret = \"Iz237BNqXCSStFetp196GwNBXWNTHmmJFit6ZcdxAwwHP17rf0\"\n",
    "access_token = \"1388816341516365825-pQddB13qZxsz3DSHqeKMW8vmHAq2OY\"\n",
    "access_token_secret = \"mSIM8JQwJi3IU1F5MF9bLWRVQIlPzD3d1LHPjCfYu74L3\"\n",
    "\n",
    "codes = {\"consumer_key\":consumer_key,\"consumer_secret\":consumer_secret,\n",
    "         \"access_token\":access_token,\"access_token_secret\":access_token_secret}\n",
    "\n",
    "with open(\"twitter.keys\",\"w\") as f:\n",
    "    json.dump(codes,f)\n",
    "    \n",
    "with open(\"twitter.keys\",\"r\") as f:\n",
    "    codes = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cr√©er une collecte en continue de tweets :\n",
    "- les strat√©gies ?\n",
    "- aller plus loin dans la compr√©hension de Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a donc plusieurs briques qui permettent de cr√©er notre traitement de donn√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Des usages plus avanc√©s de l'API : collecter un flux de tweets\n",
    "\n",
    "- un scrit qui fait une veille permanente\n",
    "- doit √™tre ex√©cut√© sur un ordinateur dans la dur√©e (serveur ?)\n",
    "- des √©l√©ments des biblioth√®ques permettant de coder ce type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API initialis√©e\n",
      "1393549207861071875\n",
      "Erreur rencontr√©e\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "import json\n",
    "\n",
    "# StreamListener class inherits from tweepy.StreamListener and overrides on_status/on_error methods.\n",
    "class StreamListener(tweepy.StreamListener):\n",
    "    def on_status(self, status):\n",
    "        print(status.id_str)\n",
    "        # if \"retweeted_status\" attribute exists, flag this tweet as a retweet.\n",
    "        is_retweet = hasattr(status, \"retweeted_status\")\n",
    "\n",
    "        # check if text has been truncated\n",
    "        if hasattr(status,\"extended_tweet\"):\n",
    "            text = status.extended_tweet[\"full_text\"]\n",
    "        else:\n",
    "            text = status.text\n",
    "\n",
    "        # check if this is a quote tweet.\n",
    "        is_quote = hasattr(status, \"quoted_status\")\n",
    "        quoted_text = \"\"\n",
    "        if is_quote:\n",
    "            # check if quoted tweet's text has been truncated before recording it\n",
    "            if hasattr(status.quoted_status,\"extended_tweet\"):\n",
    "                quoted_text = status.quoted_status.extended_tweet[\"full_text\"]\n",
    "            else:\n",
    "                quoted_text = status.quoted_status.text\n",
    "\n",
    "        # remove characters that might cause problems with csv encoding\n",
    "        remove_characters = [\",\",\"\\n\"]\n",
    "        for c in remove_characters:\n",
    "            text.replace(c,\" \")\n",
    "            quoted_text.replace(c, \" \")\n",
    "            \n",
    "        with open(\"tweets/\"+status.id_str,\"w\") as f:\n",
    "            json.dump(status._json,f)\n",
    "\n",
    "    def on_error(self, status_code):\n",
    "        print(\"Encountered streaming error (\", status_code, \")\")\n",
    "\n",
    "\n",
    "def launch():\n",
    "    try:\n",
    "        auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "        auth.set_access_token(access_token, access_token_secret)\n",
    "        api = tweepy.API(auth)\n",
    "        print(\"API initialis√©e\")\n",
    "        streamListener = StreamListener()\n",
    "        stream = tweepy.Stream(auth=api.auth, listener=streamListener,tweet_mode='extended')\n",
    "        tags = [\"islamogauchisme\"]\n",
    "        stream.filter(track=tags)\n",
    "    except:\n",
    "        print(\"Erreur rencontr√©e\")\n",
    "        #launch()\n",
    "    \n",
    "launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les limites de Tweepy, et la n√©cessit√© de passer par l'API Twitter\n",
    "- https://developer.twitter.com/en/docs/twitter-api/tweets/search/introduction\n",
    "- https://github.com/twitterdev/search-tweets-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moment 3 - Pas d'API : collecter directement des donn√©es sur internet (Emilien, 30 min)\n",
    "\n",
    "- Dans certains cas il n'y a pas d'API disponible, r√©cup√©rer directement les donn√©es par les interfaces \"standard\"\n",
    "- Utiliser requests et BeautifulSoup\n",
    "- Importance de la r√©tro-ing√©nieurie : comprendre l'architecture d'une page web\n",
    "- Diff√©rentes strat√©gies : regex ou biblioth√®ques plus avanc√©es\n",
    "- R√©cup√©rer des images ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R√©cup√©rer des notices de livres Python sur Wordcat https://www.worldcat.org/ puis mettre en forme dans un fichier\n",
    "- chercher islamo-gauchisme\n",
    "- voir la forme de l'URL https://www.worldcat.org/search?q=islamo-gauchisme&fq=&dblist=638&start=21&qt=page_number_link\n",
    "\n",
    "D√©marche : \n",
    "- r√©cup√©rer les pages de r√©sultat\n",
    "- construire un tableau des liens vers les r√©sultats\n",
    "- r√©cup√©rer chaque notice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Premi√®re √©tape regarder un peu les √©l√©ments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R√©cup√©rer une page avec requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://www.worldcat.org/search?q=islamo-gauchisme&fq=&dblist=638&start=1&qt=page_number_link\"\n",
    "\n",
    "page = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toutes les pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.worldcat.org/search?q=islamo-gauchisme&fq=&dblist=638&start=1&qt=page_number_link\n",
      "https://www.worldcat.org/search?q=islamo-gauchisme&fq=&dblist=638&start=11&qt=page_number_link\n",
      "https://www.worldcat.org/search?q=islamo-gauchisme&fq=&dblist=638&start=21&qt=page_number_link\n",
      "https://www.worldcat.org/search?q=islamo-gauchisme&fq=&dblist=638&start=31&qt=page_number_link\n",
      "https://www.worldcat.org/search?q=islamo-gauchisme&fq=&dblist=638&start=41&qt=page_number_link\n",
      "https://www.worldcat.org/search?q=islamo-gauchisme&fq=&dblist=638&start=51&qt=page_number_link\n",
      "https://www.worldcat.org/search?q=islamo-gauchisme&fq=&dblist=638&start=61&qt=page_number_link\n"
     ]
    }
   ],
   "source": [
    "pages = []\n",
    "for i in range(0,7):\n",
    "    url = \"https://www.worldcat.org/search?q=islamo-gauchisme&fq=&dblist=638&start={}&qt=page_number_link\".format(1+10*i)\n",
    "    print(url)\n",
    "    page = requests.get(url)\n",
    "    pages.append(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraire les liens : un peu de r√©troing√©nierie\n",
    "\n",
    "Plusieurs strat√©gies :\n",
    "- tous les liens et filtrer\n",
    "- respecter la structure du document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se balader dans la structure html : BeautifoulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = bs4.BeautifulSoup(pages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "liens = page.find_all(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "liens = [i.attrs[\"href\"] for i in liens if \"href\" in i.attrs and \"/oclc/\" in i.attrs[\"href\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/title/allons-nous-sortir-de-lhistoire/oclc/1100451295',\n",
       " '/title/emirats-de-la-republique-comment-les-islamistes-prennent-possession-de-la-banlieue/oclc/1140404936',\n",
       " '/title/empoisonneurs-antisemitisme-islamophobie-xenophobie/oclc/1198222296',\n",
       " '/title/islamo-gauchisme/oclc/8760612072',\n",
       " '/title/islamo-gauchisme/oclc/8932537540',\n",
       " '/title/liaisons-dangereuses-islamo-nazisme-islamo-gauchisme/oclc/1248694509',\n",
       " '/title/livre-des-indesires-une-histoire-des-arabes-en-france/oclc/1084514701',\n",
       " '/title/negationnisme-de-gauche/oclc/1099928429',\n",
       " '/title/racines-de-lislamo-gauchisme-dossier/oclc/1057453730',\n",
       " '/title/racisme-imaginaire-islamophobie-et-culpabilite/oclc/974816801'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([i.replace(\"&referer=brief_results\",\"\").replace(\"/editions?editionsView=true&referer=br\",\"\") for i in liens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = liens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.attrs[\"href\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ecrire une fonction pour r√©cup√©rer les √©l√©ments dont on a besoin sur chacune des pages ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.worldcat.org/title/islamo-gauchisme-du-pseudo-debat-a-la-realite-du-terrain/oclc/8999871764\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = bs4.BeautifulSoup(requests.get(url).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "details = page.find_all(\"div\",{\"id\":\"details\"})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = details.find_all(\"div\",{\"class\":\"abstracttxt\"})[0].text\n",
    "doctype =  details.find_all(\"tr\",{\"id\":\"details-doctype\"})[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(url):\n",
    "    page = bs4.BeautifulSoup(requests.get(url).content)\n",
    "    details = page.find_all(\"div\",{\"id\":\"details\"})[0]\n",
    "    resume = details.find_all(\"div\",{\"class\":\"abstracttxt\"})[0].text\n",
    "    doctype =  details.find_all(\"tr\",{\"id\":\"details-doctype\"})[0].text\n",
    "    return [url,resume,doctype]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.worldcat.org/title/islamo-gauchisme-du-pseudo-debat-a-la-realite-du-terrain/oclc/8999871764',\n",
       " \"\\n           Au c≈ìur de ce deÃÅbat qui prend des tournures toujours plus inattendues, j'ai eu la chance d'avoir une conversation exigeante et mesureÃÅe, afin de prendre un peu de recul. Merci aÃÄ CTRL Z pour sa confiance et, au passage, pour ses articles d'une trop rare finesse sur le mondes numeÃÅriques et les deÃÅbats de notre temps ! La conversation meneÃÅe avec la mirifique Elodie Safaris est accessible sur Youtube ou Spotify ; je vous incruste ici la version Youtube. https://www.youtube.com/embed/ausj14hU-e...\",\n",
       " '\\nDocument Type:\\nArticle\\n']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://www.worldcat.org/title/islamo-gauchisme-du-pseudo-debat-a-la-realite-du-terrain/oclc/8999871764\"\n",
    "get_info(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tout int√©grer dans un script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(0,7):\n",
    "    url = \"https://www.worldcat.org/search?q=islamo-gauchisme&fq=&dblist=638&start={}&qt=page_number_link\".format(1+10*i)\n",
    "    page = requests.get(url)\n",
    "    page = bs4.BeautifulSoup(page.content)\n",
    "    liens = page.find_all(\"a\")\n",
    "    liens = [i.attrs[\"href\"] for i in liens if \"href\" in i.attrs and \"/oclc/\" in i.attrs[\"href\"]]\n",
    "    liens = set([i.replace(\"&referer=brief_results\",\"\").replace(\"/editions?editionsView=true&referer=br\",\"\") for i in liens])\n",
    "    corpus+=list(liens)\n",
    "\n",
    "data = []\n",
    "for url in corpus:\n",
    "    data.append(get_info(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moment 4 - Plus d'API, du point de vue humain √† celui de l'ordinateur (Matthias, 15-30 min)\n",
    "\n",
    "Diff√©rents types d'API / G√©n√©ralisation\n",
    "\n",
    "\n",
    "Les bilioth√®ques peuvent avoir un notion d'API.\n",
    "\n",
    " - Par exemple Vaex, et Dask-dataframe sont deux biblioth√®ques qui expose une API similaire a Pandas, inndiquant \n",
    "\n",
    "Diff√©rentes version d'API\n",
    "\n",
    " - les biblioth√®ques vont souvent essayer de garder un API similaire pour des version proche. En g√©n√©ral une biblioth√®que version X.y.z and X.a.b vont avoir des API \"compatible\".\n",
    "\n",
    " - Les API web parfois on la possibilit√© de choisir la version de l'API avec laquelle on interagit.\n",
    "\n",
    "usages plus compliqu√©s : async/await\n",
    "\n",
    " - Pour la collection massive de donn√©es, cherchez async/await qui permet de r√©cuperer des r√©sultats de mani√®re concourante. \n",
    " - Rate limits\n",
    "Cr√©er sa propre API ? Garder la signature des fonctions / https://fastapi.tiangolo.com/\n",
    "\n",
    "\n",
    "\n",
    "S3 / Lien avec les bases de donn√©es ? Exemple de sqlite ? https://docs.python.org/3/library/sqlite3.html \n",
    "\n",
    "Quelques liens\n",
    "\n",
    "- Livres Gallica : https://api.bnf.fr/fr/wrapper-python-pour-les-api-gallica / https://github.com/ian-nai/PyGallica\n",
    "- Vid√©os : https://pypi.org/project/python-youtube/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quelques Termes  quand on parle d'API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Pas besoin de retenir, mais au cas o√π). \n",
    "\n",
    "REST est un type d'API assez courant, utilis√© pour modifier des \"documents\" ‚Äì¬†par example, tweets, page wikipedia.\n",
    "  - GET (requests.get) pour obtenir un document \n",
    "  - POST (request.post) g√©n√©rique en parculier si il y a des informations qui ne sont pas dans l'URL √† envoyer au serveur.\n",
    "  - PUT (request.put) pour t√©l√©verser un document\n",
    "  - PATCH (request.patch) pour modifier un document\n",
    "  - HEAD (request.head) recevoir  uniquement les metadonn√©es\n",
    "  - DELETE (request.delete)\n",
    "  \n",
    "Dans la documentation on Anglais il peut √™tre difficile de rep√©rer que ces termes sont des termes techniques.\n",
    "\n",
    "\n",
    "Status Code:\n",
    "  - 2xx : Tout vas bien\n",
    "  - 3xx (souvent 304) : Probl√®me d'autentification\n",
    "  - 4xx : √ßa existe pas.\n",
    "  - 5xx : donn√©es incorrect ou le serveur a plant√©.\n",
    "  \n",
    "HTTP(s):\n",
    "\n",
    "  - body and header. (encryted) \n",
    "     - header = Metadata\n",
    "     - body = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date                           Thu, 20 May 2021 05:23:36 GMT\n",
      "Server                         mw1406.eqiad.wmnet\n",
      "X-Content-Type-Options         nosniff\n",
      "P3p                            CP=\"See https://en.wikipedia.org/wiki/Special:CentralAutoLogin/P3P for more info.\"\n",
      "X-Search-Id                    39vc56oobudouvqoqe48u5tyt\n",
      "X-Opensearch-Type              comp_suggest\n",
      "X-Frame-Options                SAMEORIGIN\n",
      "Content-Disposition            inline; filename=api-result.json\n",
      "Vary                           Accept-Encoding,Treat-as-Untrusted,X-Forwarded-Proto,Cookie,Authorization\n",
      "Expires                        Thu, 20 May 2021 08:23:36 GMT\n",
      "Cache-Control                  max-age=10800, s-maxage=10800, public\n",
      "X-Request-Id                   d6b4e1be-2c60-4568-bb2d-07d9b429400f\n",
      "Content-Type                   application/json; charset=utf-8\n",
      "Content-Encoding               gzip\n",
      "Age                            19\n",
      "X-Cache                        cp4030 miss, cp4029 hit/1\n",
      "X-Cache-Status                 hit-front\n",
      "Server-Timing                  cache;desc=\"hit-front\", host;desc=\"cp4029\"\n",
      "Strict-Transport-Security      max-age=106384710; includeSubDomains; preload\n",
      "Report-To                      { \"group\": \"wm_nel\", \"max_age\": 86400, \"endpoints\": [{ \"url\": \"https://intake-logging.wikimedia.org/v1/events?stream=w3c.reportingapi.network_error&schema_uri=/w3c/reportingapi/network_error/1.0.0\" }] }\n",
      "NEL                            { \"report_to\": \"wm_nel\", \"max_age\": 86400, \"failure_fraction\": 0.05, \"success_fraction\": 0.0}\n",
      "Permissions-Policy             interest-cohort=()\n",
      "Set-Cookie                     WMF-Last-Access=20-May-2021;Path=/;HttpOnly;secure;Expires=Mon, 21 Jun 2021 00:00:00 GMT, WMF-Last-Access-Global=20-May-2021;Path=/;Domain=.wikipedia.org;HttpOnly;secure;Expires=Mon, 21 Jun 2021 00:00:00 GMT, GeoIP=US:CA:Merced:37.33:-120.50:v4; Path=/; secure; Domain=.wikipedia.org\n",
      "X-Client-IP                    73.90.141.68\n",
      "Accept-Ranges                  bytes\n",
      "Content-Length                 226\n",
      "Connection                     keep-alive\n"
     ]
    }
   ],
   "source": [
    "for k,v in response.headers.items():\n",
    "    print(f\"{k:<30}\", v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      \n",
    " \n",
    "\n",
    "Reseaux-Sociaux:\n",
    "\n",
    "  - GraphQL: Renvoie uniquement les elements demand√©s + r√©cursif. Tout ce qui est difficile en tabulaire.\n",
    "    \"@pyshs1 -> Tweet avec le plus de like -> utilisateur qui on lik√© ce tweet -> nom d'utilisateur\".\n",
    "      \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "mimetype": "text/python",
   "name": "Python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
